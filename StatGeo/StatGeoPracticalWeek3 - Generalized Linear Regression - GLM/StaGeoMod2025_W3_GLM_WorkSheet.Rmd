---
title: "GLMs Lab: Count & Proportion Data"
subtitle: "A full workflow with interpretation, diagnostics, and model simplification"
author: "Søren Meier"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    self_contained: yes
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,error = TRUE)
set.seed(123)
library(tidyverse)
library(MASS)        # glm.nb, stepAIC
```

## Learning goals

-   Choose and fit appropriate GLMs for **count** (Poisson / Negative
    Binomial) and **proportion** (Binomial / quasi-Binomial) data.\
-   Interpret coefficients: **rate ratios** (counts) and **odds ratios**
    (proportions).\
-   Assess fit via **deviance, dispersion**, residual diagnostics, and
    **model simplification** (LRT, AIC).\
-   Make predictions and visualise with **ggplot2**.

::: callout-note
**What you’ll submit**:

An HTML version of this exercise by the end of the practial session
:::

## Lab Timeline (2 hours)

| Time | Activity |
|----|----|
| 0:00–0:15 | **Introduction to GLMs** – recap linear model limits, why we need GLMs. |
| 0:15–0:35 | **Exploratory Data Analysis (EDA)** – plotting counts and proportions with ggplot2. |
| 0:35–1:05 | **Count data example (Negative Binomial GLM)**: simulate / load data, fit Poisson, check overdispersion, refit with `glm.nb()`, interpret coefficients, plot fitted vs observed. |
| 1:05–1:25 | **Proportional data example (Binomial GLM)**: fit model using `glm(..., family=binomial)`, interpret log-odds & odds ratios, check residuals, plot fitted probabilities with CIs. |
| 1:25–1:40 | **Model simplification & comparison**: use `anova()`, `drop1()`, and AIC for model selection. |
| 1:40–1:50 | **Diagnostics**: residual vs fitted plots, dispersion checks, discussion of overdispersion and quasi families. |
| 1:50–2:00 | **Wrap-up**: Key takeaways, when to use Poisson, Negative Binomial, Binomial/Quasi-Binomial, interpretation of multiplicative vs additive effects. |

## Part A — Count data (≈ 60 min)

### A1. Simulate “roadkills”-style counts

::: {.alert .alert-info}
**Task.**

Create a simulation that generates synthetic bird count data based on
three environmental predictors:

1)  `OPEN.L`: Percentage of open land (0-100%)
2)  `D.PAR`K\`: Distance to nearest park (0-5000 meters)
3)  `L.WAT.C`: Length of nearby watercourses (0-5 km)

**Your Mission**

***Step 1*****: Set up the simulation parameters**

Generate data for 600 observation sites Create realistic ranges for each
predictor variable using appropriate random distributions

***Step 2*****: Define the ecological relationships**

The true relationship follows this pattern:

1)  More open land → fewer birds (coefficient: -0.010)
2)  Greater distance to parks → fewer birds (coefficient: -0.00012)
3)  More watercourse length → more birds (coefficient: +0.18)
4)  Baseline log-abundance: 1.2

***Step 3*****: Generate realistic count data**

Use the linear predictor to calculate expected abundance Add ecological
realism by incorporating overdispersion ($\theta = 4$) Generate final
bird counts using an appropriate count distribution

***Step 4*****: Organize and explore your data**

Combine all variables into a clean data frame Examine the structure and
summary of your simulated dataset
:::

```{r}
# Simulate predictors
# Step 1: Simulation setup
set.seed(123)  # For reproducible results
n <- 600       # Number of sites - 600

# Step 2: Generate predictor variables
OPEN.L <- runif(n, 0, 100)     # Hint: uniform distribution, 0 to 100
D.PARK <- runif(n, 0, 5000)     # Hint: uniform distribution, 0 to 5000
L.WAT.C <- runif(n, 0, 5)    # Hint: uniform distribution, 0 to 5

# Step 3: Create the ecological model
eta <- 1.2+(-0.010*OPEN.L)+(-0.00012*D.PARK)+(0.18*L.WAT.C)        # Linear predictor combining all effects
                  # Beta_OPEN.L = 0.010
                  # Beta_D.PARK = 0.00012
                  # Beta_L.WAT.C = 0.18
mu <- exp(eta)         # Transform eta to expected count scale
theta <- 4      # Overdispersion parameter (4)

# Step 4: Generate observed counts
TOT.N <- rnbinom(n,mu=mu,size=theta)      # Hint: use a random negative binomial distribution

# Step 5: Create final dataset merging TOT.N, OPEN.L, D.PARK, & L.WAT.C
datC <- tibble(TOT.N, OPEN.L, D.PARK, L.WAT.C)
glimpse(datC)
```

### A2. Poisson GLM, dispersion check, quasi-Poisson

::: {.alert .alert-info}
**Task.**

Fit a Poisson GLM as a baseline; compute dispersion. If dispersion \>\>
1, fit a quasi-Poisson to obtain robust SEs (note: no AIC for
quasi-families).

**Your Mission**

***Step 1*****: Fit a baseline Poisson GLM**

Use your simulated bird count data (TOT.N) with all three environmental
predictors to establish a starting model.

***Step 2*****: Check for overdispersion**

Calculate the dispersion parameter by comparing residual deviance to
degrees of freedom. A value \>\> 1 indicates overdispersion.

***Step 3*****:\* Apply quasi-Poisson correction if needed**

If overdispersion is detected, fit a quasi-Poisson model to obtain
robust standard errors that account for extra variability.

***Step 4*****: Compare model results**

Examine how overdispersion correction affects coefficient estimates,
standard errors, and statistical significance
:::

```{r}
# Fit Poisson GLM
m_pois <- glm(TOT.N ~ OPEN.L + D.PARK + L.WAT.C, 
              family = poisson, 
              data = datC)

# Examine the results
summary(m_pois)

# Calculate dispersion parameter
disp_pois <- m_pois$deviance / m_pois$df.residual
print(paste("Dispersion parameter:", round(disp_pois, 2)))

# Fit quasi-Poisson GLM
m_qp <- glm(TOT.N ~ OPEN.L + D.PARK + L.WAT.C, 
            family = quasipoisson, 
            data = datC)

# Compare results
summary(m_qp)

# Extract coefficients and SEs
pois_coef <- summary(m_pois)$coefficients[, 1:2]
qp_coef <- summary(m_qp)$coefficients[, 1:2]

# Create comparison
comparison <- data.frame(
  Predictor = rownames(pois_coef),
  Poisson_Coef = pois_coef[, 1],
  Poisson_SE = pois_coef[, 2],
  QuasiPois_Coef = qp_coef[, 1],
  QuasiPois_SE = qp_coef[, 2],
  SE_Inflation = qp_coef[, 2] / pois_coef[, 2]
)

print(comparison)
```

::: {.alert .alert-success}
**Question**

1)  **Model Selection**: When would you choose quasi-Poisson over
    Poisson?

    When you have overdispersion in your data / when you detect
    overdispersion in your model.

2)  **Limitations**: Why can't you calculate AIC for quasi-families?

    AIC estimations do not account for models with overdispersion -
    there is no likelihood function for quasi-families - we can only
    calculate a restricted maximum likelihood.

3)  **Alternatives**: What other approaches could handle overdispersion?
    (*Hint*: negative binomial)

    Using a negative binomial model - this is a less restrictive model
    that allows for more dispersion than the standard poisson model.

*Biological Interpretation:*

1)  Which environmental factor has the strongest effect on bird counts?

    The length of nearby watercourses: this factor has the highest z
    value (but it should be noted that the units of the factors are
    different - this could complicate interpretations when looking at
    just the values of the coefficients).

<!-- -->

1)  How do you interpret the coefficient for `D.PARK`?

    The coefficient is -0.00013, which means that a increase of 1 meter
    in distance to the nearest park decreases the log abundance of birds
    by 0.00013 and the abundance of birds by (exp(-0.00013)=0.999) 0.1%.
:::

### A3. Negative Binomial GLM & comparison

::: {.alert .alert-info}
**Task.**

Fit a Negative Binomial GLM and compare to Poisson using AIC. Prefer NB
if it provides a better likelihood-penalised fit.

**Your Mission**

***Step 1*****: Fit a Negative Binomial GLM**

Use the `glm.nb()` function to fit a negative binomial model that
explicitly accounts for overdispersion by estimating a dispersion
parameter.

***Step 2*****: Examine the model summary**

Review the coefficient estimates, standard errors, and the estimated
theta (dispersion) parameter to understand how NB differs from Poisson.

***Step 3*****: Compare models using AIC**

Calculate AIC values for both Poisson and Negative Binomial models to
determine which provides better model fit penalized for complexity.

***Step 4*****: Make an informed model choice**

Select the preferred model based on AIC comparison and biological
interpretability of results.
:::

```{r}
library(MASS)  # Required for glm.nb()

m_nb <- glm.nb(TOT.N ~ OPEN.L + D.PARK + L.WAT.C,
              data = datC)
summary(m_nb)

# Compare models using AIC
AIC(m_pois, m_nb)
```

::: {.alert .alert-success}
**Questions**

1.  **Theta parameter**: What is the estimated theta ($\theta$) value in
    your negative binomial model? How does this compare to the true
    value ($\theta = 4$) used in your simulation?

4.018 in the negative binomial model. The true value is 4, and the
estimated value is therefore very close to the true value.

2.  **Standard errors**: Compare the standard errors between the Poisson
    and Negative Binomial models. Which model has larger standard errors
    and why?

The standard errors for the negative binomial model are larger. This is
because the NB model actually accounts for overdispersion, and
overdispersion results in larger standard errors - the poisson model
underestimates the standard errors.

2.  **AIC comparison**: Which model has the lower AIC value? What does
    this tell you about model fit?

The negative binomial model has a lower AIC value (and also lower
residual deviance). The model fit of this model is therefore better than
the poisson model.

4.  **Coefficient interpretation**: Are the coefficient signs and
    magnitudes similar between Poisson and NB models? What does this
    suggest about the robustness of your ecological conclusions?

The magnitudes are very similar, and the coeffcient signs are also the
same. The robustness is therefore very strong, as the same conclusions
could be made from different models.

**Model choice**: Based on your AIC comparison, which model would you
choose for inference and why?

I would choose the negative binomial model, as it has a lower AIC value,
which means it will estimate more accurate values.

6.  **Practical implications**: How might using the wrong model (Poisson
    vs. NB) affect your conclusions about which environmental factors
    significantly influence bird abundance?

Using the wrong model could wrongly estimate the magnitudes of the
coefficients of the factors, which in turn affects the conclusions of
the influence of the different factors.
:::

### A4. Model simplification (drop-one, stepAIC)

::: {.alert .alert-info}
**Task.**

Simplify the NB model: inspect drop-one Chi-square tests, then apply
stepAIC to seek a parsimonious model.

**Your Mission**

***Step 1: Conduct drop-one tests***

Use `drop1()` with Chi-square tests to evaluate the individual
contribution of each predictor when removed from the full model.

***Step 2: Apply automated model selection***

Use `stepAIC()` to systematically search for the most parsimonious model
by comparing AIC values across different predictor combinations.

***Step 3: Examine the final simplified model***

Review the summary of the selected model to understand which predictors
were retained and their statistical significance.

***Step 4: Interpret coefficients on the natural scale***

Transform log-scale coefficients to rate ratios (multiplicative effects)
for easier ecological interpretation.
:::

```{r}
# Step 1: Perform drop-one Chi-square tests
drop1(m_nb, test = "Chi")    # Test individual predictor contributions

# Step 2: Apply stepwise AIC model selection  

m_nb_step <- stepAIC(m_nb, trace = TRUE)    # Find most parsimonious model
                                          # trace = FALSE suppresses output

# Step 3: Examine the simplified model
summary(m_nb_step)    # Review final model structure and significance

# Step 4: Calculate rate ratios (multiplicative effects)
exp(coef(m_nb_step))    # Transform coefficients to natural scale
                  # exp() converts log-effects to multiplicative effects
```

::: {.alert .alert-success}
**Questions**

## Interpretation Questions

1.  **Drop-one results**: Which predictor(s) show the highest Chi-square
    values in the drop-one test? What does this indicate about their
    importance?

All the predictors have significant Chi-square values, which means that
they all are important in the model and should be retained.

The predictor OPEN.L has the highest LRT value and is therefore the most
important to keep in the model.

2.  **Model selection outcome**: Did stepAIC retain all three
    predictors, or was the model simplified? Which variables (if any)
    were dropped?

All predictors were retained.

3.  **AIC improvement**: Compare the AIC of the final stepped model with
    the original full model. Is there evidence that simplification
    improved the model?

No simplification was done.

4.  **Rate ratio interpretation**: Looking at the `exp(coef())` output,
    how do you interpret the multiplicative effects? For example, what
    happens to bird abundance for every 1% increase in open land?

For every 1% increase in open land, the abundance of birds decreases by
1% (coef=0.990).

For every 1 meter of distance to the nearest park, the abundance of
birds decrease by 0.1% (coef=0.999)

For every 1 km increase in length of nearby watercourses, the abundance
of birds increases by 20% (coef=1.204)

5.  **Ecological significance**: Which environmental factor has the
    strongest multiplicative effect on bird abundance? How would you
    translate this into management recommendations?

The length of nearby watercourses has the strongest effect. In
management recommendations, increasing the length of nearby watercourses
should be considered the strongest, as this will increase the abundance
of birds the most.

5.  **Statistical vs. biological significance**: Are all retained
    predictors both statistically significant (p \< 0.05) and
    ecologically meaningful? How do you distinguish between the two?

All retained predictors are statistically significant.
:::

### A5. Predictions (response scale) & CI plot

::: {.alert .alert-info}
**Task.**

Produce fitted means and 95% confidence bands on the response scale for
a key covariate (hold others at medians); plot observed counts + fitted
curve.

**Your Mission**

***Step 1: Create a prediction grid***

Generate a sequence of values for your focal predictor (`OPEN.L`) while
holding other predictors at their median values to isolate the effect of
interest.

***Step 2: Generate model predictions***

Use the simplified model to predict bird counts across the range of open
land percentages, including standard errors for uncertainty
quantification.

***Step 3: Transform predictions to response scale***

Convert log-scale predictions and confidence intervals back to the count
scale for meaningful interpretation.

***Step 4: Create a publication-ready plot***

Combine observed data points with model predictions and confidence bands
to visualize the relationship.
:::

```{r}
# Step 1: Create prediction grid for focal variable
gridC <- tibble(
  OPEN.L = seq(min(datC$OPEN.L), max(datC$OPEN.L), length.out = 300),  # Sequence of 300 elements from min to max OPEN.L
  D.PARK = median(datC$D.PARK),                                        # Hold D.PARK at median value
  L.WAT.C = median(datC$L.WAT.C, na.rm = TRUE)                       # Hold L.WAT.C at median value
)

# Step 2: Generate predictions with standard errors
pl <- predict(m_nb_step, newdata = gridC, type = "link", se.fit = TRUE)        # Get link-scale predictions + SEs
                                                                      # type = "link" gives log-scale
                                                                      # se.fit = TRUE includes SEs

# Step 3: Transform to response scale with confidence intervals
gridC <- gridC |>
  mutate(
    mu = exp(pl$fit),                           # Transform fitted values to count scale
    lo = exp(pl$fit - 1.96*pl$se.fit),        # Lower 95% CI boundary
    hi = exp(pl$fit + 1.96*pl$se.fit)         # Upper 95% CI boundary
  )

# Step 4: Create the prediction plot
ggplot(datC, aes(x=OPEN.L, y=TOT.N)) +                                         # Plot original data
  geom_point(alpha = 0.5) +                                          # Add semi-transparent points
  geom_ribbon(data = gridC, aes(x = OPEN.L, ymin = lo, ymax = hi), 
              alpha = 0.5,
              inherit.aes = FALSE) +                    # Add confidence band
  geom_line(data = gridC, aes(x = OPEN.L, y = mu), 
            linewidth = 1, inherit.aes = FALSE) +                  # Add fitted line
  labs(title = "Negative binomial GLM: fitted counts vs % open land",
       x = "Open land percentage",
       y = "Abundance of birds") +                                                  # Add informative labels
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

1.  **Prediction grid setup**: Why do we hold D.PARK and L.WAT.C at
    their median values when creating predictions for OPEN.L? What would
    happen if we used different values?

Because D.PARK and L.WAT.C also affect the abundance of birds - to
predict only the effect of OPEN.L, we have to hold the others values
constant. We use the median values to get the

2.  **Link vs. response scale**: Why do we first predict on the "link"
    scale and then transform using `exp()`, rather than predicting
    directly on the response scale?

Because the model is made in the link space and ensures predictions of
only valid values. Predicting on the response scale directly could yield
invalid values (e.g. negative values of open land percentage)

3.  **Confidence interval interpretation**: What do the confidence bands
    around the fitted line represent? How would you explain this to a
    non-statistician?

The confidence bands indicate a 95% confidence interval. This is a
interval around the predicted value where we are 95% of the time expect
the true value to contained within, if we repeat this experiment many
times.

4.  **Ecological pattern**: Describe the relationship between open land
    percentage and bird counts shown in your plot. Does this match your
    biological expectations?

AS the open land percentage increases, the abundance of birds decreases.
This matches our expectations.

5.  **Model fit assessment**: Looking at how well the fitted line and
    confidence bands capture the observed data points, would you say
    this model provides a good fit? What might you look for to assess
    this?

The model fit seems to capture the general trend quite well.

The residual deviance of the model compared to the null deviance gives
an indication of how better the model is compared to the null model with
none of the predictors - a lower residual deviance compared to the null
deviance indicate better models.

6.  **Practical applications**: How could you use this plot to inform
    urban planning decisions about bird conservation?

To conserve as many birds as possible, the land should be less open -
urban planning should include more trees/parks.
:::

### A6. Diagnostics: residuals & influence

::: {.alert .alert-info}
**Task.**

Inspect deviance/Pearson residuals vs fitted and influence diagnostics.
Look for structure (misspecification) and high-leverage points.

**Your Mission**

***Step 1: Extract diagnostic measures***

Create a comprehensive dataset containing fitted values, residuals, and
influence measures from your negative binomial model to assess model
assumptions and identify problematic observations.

***Step 2: Create residuals vs fitted plot***

Plot deviance residuals against fitted values to check for patterns that
might indicate model misspecification, non-linearity, or
heteroscedasticity.

***Step 3: Examine influence diagnostics***

Create a leverage vs Cook's distance plot to identify observations that
have high influence on model parameters or are potential outliers.

***Step 4: Interpret diagnostic patterns***

Assess whether the model adequately captures the data structure and
identify any observations requiring further investigation.
:::

```{r}
# Step 1: Extract all diagnostic measures into a tibble
diag_nb <- tibble(
  fitted  = fitted(m_nb_step),                           # Extract fitted values (predicted means)
  devres  = residuals(m_nb_step, type = "deviance"),         # Extract deviance residuals
  pearson = residuals(m_nb_step, type = "pearson"),         # Extract Pearson residuals  
  hat     = hatvalues(m_nb_step),                       # Extract leverage values (diagonal of hat matrix)
  cooks   = cooks.distance(m_nb_step)                   # Extract Cook's distance (influence measure)
)

# Step 2: Create deviance residuals vs fitted values plot
ggplot(diag_nb, aes(fitted,devres)) +                      # Plot fitted vs deviance residuals
  geom_hline(yintercept = 0, linetype = 2) +  # Add horizontal reference line at y=0 (dashed)
  geom_point(alpha = 0.5) +                       # Add semi-transparent points (0.5)
  geom_smooth(se = FALSE) +                         # Add smooth trend line without confidence bands
  labs(title = "Negative binomial GLM: deviance residuals vs fitted", 
       x = "Fitted mu", 
       y = "Deviance residual") +                               # Add descriptive labels
  theme_minimal()

# Step 3: Create leverage vs Cook's distance plot
ggplot(diag_nb, aes(hat, cooks)) +                      # Plot leverage vs Cook's distance
  geom_point(alpha = 0.5) +                       # Add semi-transparent points (0.6)
  labs(title = "Influence: leverage vs Cook's distance", 
       x = "Leverage", 
       y = "Cooks distance") +                               # Add descriptive labels
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

1.  **Residual types**: What's the difference between deviance and
    Pearson residuals, and why might we prefer deviance residuals for
    GLM diagnostics?

*Answer here*

2.  **Residuals vs fitted interpretation**: In your deviance residuals
    plot, what pattern would indicate a well-fitting model? What
    patterns would suggest problems?

The plotted line should be as flat as possible. A curved line or a line
far away from the flat line at y=0 could indicate problems.

3.  **Reference line importance**: Why do we include a horizontal dashed
    line at y=0 in the residuals plot, and what does it represent?

y=0 indicates a model fit where the residuals are 0, which is a perfect
model. We include it to see how close our model is to a perfect model,
which is a way to asses how good our model is.

4.  **Leverage interpretation**: What does high leverage indicate about
    an observation, and why should we be concerned about high-leverage
    points?

High leverage of an observation indicates that a point influences the
model fit considerably more than other points / that including this
observation changes the model fit considerably. High leverage points can
produce wrong results/conclusions.

5.  **Cook's distance threshold**: Cook's distance measures overall
    influence. What general threshold is often used to identify
    potentially problematic observations, and why?

The general threshold is if Cooks distance is over 4/# observations.
Values over this threshold have a high influence and could therefore be
problematic for making accuracte conclusions.

6.  **Diagnostic integration**: How would you use both plots together to
    identify the most concerning observations? What would be the "worst
    case" combination of leverage and Cook's distance?

Observations with high values of both leverage and Cooks distance would
be very problematic.

7.  **Model adequacy assessment**: Based on your diagnostic plots, would
    you conclude that the negative binomial GLM adequately fits your
    simulated data? What evidence supports this conclusion?

The negative binomial GLM fits the simulated data quite well. The
deviance residual plot shows a relatively flat line with no distinct
pattern, and the leverage plot shows that almost all observations have
very low leverage and Cooks distance. Our coeffecients are also very
close to the true values, and are all statistically significant.
:::
