---
title: "Hands-on Practical: Generalised Additive Models (GAMs) with Bioluminescence"
subtitle: "StatGeoMod2025 — 1.5-hour practical"
author: "Søren Meier"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    self_contained: yes
    toc: true
    toc_depth: 3
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

## Setup

```{r}
#| label: setup
#| include: true
#| eval: true
#| echo: false
#| message: false

# Runng options
knitr::opts_chunk$set(echo = TRUE,error=TRUE)

# Packages
library(tidyverse)
library(mgcv)
library(broom)
library(patchwork)

# Data: ISIT 
ISIT <-  read_table("ISIT.txt")

# For reproducibility
set.seed(123)




```

## Part A — Why GAMs? (Basics & intuition) (10 min)

::: {.alert .alert-info}
**Task A1 (concept).**

A GAM replaces a single global slope with a **smooth function**
$f(x): Y_i = \alpha + f(X_i) + \varepsilon_i$.

In `mgcv`, `s(x)` builds $f(x)$ from spline bases, and smoothness is
selected automatically (REML/GCV).
:::

::: {.alert .alert-info}
**Task A2 (visual).**

**What you’ll do (and why):**

1.  **Prepare the data.** Select only Station 16 from the ISIT dataset
    and create a clean dataframe with three variables: `Station`,
    `Depth`, and `Sources`.\
2.  **Inspect the structure.** Use `glimpse()` to confirm that `Depth`
    is numeric (continuous predictor) and `Sources` is count data
    (non-negative integers). This step helps you check if the data type
    matches the modelling assumptions later.\
3.  **Visualise.** Make a scatterplot of Sources vs Depth using
    `ggplot2`. Adjust the transparency (`alpha = 0.7`) to help visualise
    overlapping points. Add informative labels and a minimal theme.\
4.  **Interpret.** After plotting, pause to describe what you see:
    -   Is the pattern approximately linear, or clearly non-linear?\
    -   Are there regions of the depth gradient where Sources increase,
        peak, or decline?\
    -   Would fitting a straight line capture the overall shape, or
        would it miss important features?

This visual exploration sets the stage for why GAMs are needed: we often
encounter curved, irregular, or locally varying relationships that
polynomials or linear fits cannot capture well.
:::

```{r}
## Fill in the blanks (`___`) in the code below.
#| label: A2-plot
# Prepare Station 16
dat <- ISIT |>
  dplyr::filter(Station == 16) |>
  transmute(Station,
            Depth   = SampleDepth,
            Sources = Sources)

# Quick look
glimpse(dat)

# Visualise
ggplot(dat, aes(Depth, Sources)) +
  geom_point(alpha = 0.7) +
  labs(title = "Bioluminescence vs Depth (Station 16)",
       y = "Bioluminescence", x = "Depth") +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**.

-   What overall pattern do you see in the scatterplot?
    -   Bioluminescence goes down rapdily with increasing depth, but the
        decrease slows and becomes asymptomatic at the highest depths.
-   If you fitted a straight line here, what would it capture well, and
    what would it miss?
    -   It would capture the decrease - but it would miss the details of
        the patterns (that the decrease is not the same across all
        depths)
-   Can you identify parts of the depth range where the relationship
    changes direction (e.g., rises then falls)?
    -   There could be a increase at 1000-2000 or slightly at 3000-4000
        meters, but it is hard to tell. Overall the trends is decreasing
        across all depths.
-   How might this motivate the need for flexible smoothers in GAMs?
    -   GAMs can capture the intricate changing pattern across depth,
        that a linear model would miss.
:::

## Part B — First GAM vs Polynomial (Runge motivation) (20 min)

::: {.alert .alert-info}
**Task B1 — Fit your first GAM (cubic regression spline)**

**What you’ll do (and why):**

-   **Model:** Fit a GAM to `Sources ~ s(Depth)` using a **cubic
    regression spline** (`bs = "cr"`).\
-   **Smoothing selection:** Use **REML** (`method = "REML"`) so the
    model estimates the amount of smoothing (λ) automatically—balancing
    wiggliness and over-penalisation.\
-   **Complexity (edf):** Inspect **effective degrees of freedom (edf)**
    in `summary()` to gauge curve flexibility ($\approx$ 1 is almost
    linear; larger edf = more curvature).\
-   **Diagnostics:** Use `gam.check()` to examine residual patterns, QQ
    plot, scale–location, and the **k-index** (basis adequacy). A **low
    k-index** suggests increasing `k` (the basis dimension) and
    refitting.\
-   **Outcome:** You’ll see how GAMs adapt to the clear non-linearity in
    the bioluminescence vs depth relationship (Station 16) without
    resorting to high-degree polynomials.

**Hints for robust modelling:** - Start with the default `k` (mgcv
chooses a sensible basis size), then **only** increase it if
`gam.check()` warns (k-index \< \~1).\
- Prefer **REML** over GCV for smoother, more stable fits in many
practical settings.\
- Always validate: smooth fit $\neq$ good residuals—check both.
:::

```{r}
#| label: B1-gam-student
#| echo: true
# Fill in the blanks (___) and then set eval: true to run
# Goal: Fit a GAM with a cubic regression spline and inspect diagnostics

# 1) Fit the model (use REML for smoothing selection)
m_gam <- gam(Sources ~ s(Depth, bs = "cr"), data = dat, method = "REML")

# 2) Summarise: check edf, significance of s(), adjusted R^2, and REML score
summary(m_gam)
plot(m_gam)

# 3) Diagnostics: residual patterns and k-index
par(mfrow = c(2, 2))
gam.check(m_gam)
par(mfrow = c(1, 1))
```

::: {.alert .alert-success}
**Question**

-   **Shape check:** What does the fitted smooth suggest about how
    `Sources` changes with `Depth` (e.g., monotonic, hump-shaped,
    thresholds)?
    -   It suggests a hump-shaped curve at 1400 meters, while the rest
        of the shape has a decreasing trend.\
-   **Complexity:** What is the reported **edf**, and how would you
    interpret that value in plain language?
    -   The edf is 8.5. This value suggests that the smoother uses
        moderate curvature, and is not linear.\
-   **Adequacy:** Does `gam.check()` indicate any **patterned
    residuals** or **basis inadequacy** (k-index warnings)? If yes, what
    would you try next?
    -   There seems to be some pattern in the residuals, where residuals
        are higher than expected at lower values and lower than expected
        at higher values.

    -   The k-index is 1.04, and therefore very close to 1.

    -   These results suggests increasing the k value to get more
        curvature in the smoother in the model.\
-   **Comparative lens:** In what ways would a polynomial of degree 4–6
    likely behave differently at the **edges** of the `Depth` range
    (Runge-type behaviour)?
    -   Polynomials of higher degrees tend to have bad fits at the edges
        of predictor values, with the risk of predicting nonsensical
        values (fx negative values for a Poisson model with counts) -
        the edges often have very high or low values.\
:::

::: {.alert .alert-info}
**Task B2 — Plot the GAM on the response scale with uncertainty**

**What you’ll do (and why):**

-   **Purpose:** Turn the fitted GAM into an interpretable plot on the
    **response scale**, so students see predicted *Sources* against
    *Depth* in real units.
-   **Prediction grid:** Create a fine, evenly spaced depth grid to
    avoid jagged lines from irregular sampling.
-   **Uncertainty:** Use
    `predict(..., se.fit = TRUE, type = "response")` to obtain fitted
    values and their **standard errors** on the response scale. Add a
    ribbon for an approximate **95% pointwise interval** using
    `fit ± 2 * se.fit`.
-   **Visual clarity:** Overlay the fitted line and ribbon on the raw
    data to contrast modelled trend vs. observed scatter.

**Key ideas:** - `type = "response"` ensures predictions are on the
scale of *Sources* (not the link/linear predictor). - The ribbon is
**pointwise**, not simultaneous; it’s excellent for teaching
uncertainty, but do not over-interpret it as a joint interval over the
whole curve. - Dense grids (e.g., 300–500 points) give smooth lines
without oversmoothing the model itself.
:::

```{r}
#| label: B2-plot-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Plot the GAM fit on the response scale with ~95% pointwise intervals

# 1) Create a prediction grid over Depth
newd <- tibble(Depth = seq(min(dat$Depth), max(dat$Depth), length.out = 400))

# 2) Predict on the response scale with standard errors
pred_gam <- bind_cols(
  newd,
  as_tibble(predict(m_gam, newd, se.fit = TRUE, type = "response"))
)

# 3) Plot: points, fitted line, and ribbon for uncertainty
ggplot(dat, aes(Depth, fit)) +
  geom_point(alpha = 0.3) +
  geom_line(data = pred_gam, aes(y = fit), linewidth = 1) +
  geom_ribbon(data = pred_gam,
              aes(y = fit,
                  ymin = fit - 2*se.fit,
                  ymax = fit + 2*se.fit),
              alpha = 0.2) +
  labs(title = "GAM fit: cubic regression spline (mgcv::gam)",
       subtitle = "Line = fitted; band $\approx$ 95% pointwise interval",
       y = "Bioluminescence") +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **Scale check:** Why is `type = "response"` preferable here compared
    to `type = "link"`?
    -   Because getting the predictions on the response scale results in
        easier interpretations of the predictions.
-   **Uncertainty:** What does the shaded band represent, and where does
    it widen most? Why might that be?
    -   A 95% confidence interval for each prediction point. It widens
        the most at the start of the hump and at the largest depth
        values. This may be due to the distribution of the collected
        depth values - they are collected in small clusters of values at
        these places, and there are not actual data for values between
        these clusters.\
-   **Edge behaviour:** Do you see wider intervals near minimum/maximum
    depth? How does sampling density play a role?
    -   Near maximum depth - as mentioned, the sampling density is not
        spread out at the larger depths.\
-   **Interpretation:** Identify depth ranges where the fitted curve
    rises, plateaus, or falls. How would a straight-line model mislead
    here?
    -   The fitted curve rises from 1100 to 1900 meters and falls from 0
        to 1100 and from 1900 to 3000 meters. There could be a plateau
        from 3000 and onwards, but a slight increase could be identified
        from 3000 to 4000, but this might just be part of the random
        fluctuation in the plateau.

    -   A straight line would not capture any of these naunces of rises,
        falls, and plateaus, and would only predict a consistent fall
        across all depths.
:::

::: {.alert .alert-info}
**Task B3 — Compare GAM with polynomial regressions (degrees 2–6)**

**What you’ll do (and why):**

-   **Purpose:** Fit a set of **global polynomial** regressions (degrees
    2–6) and compare them with the GAM fit from Task B1–B2.\
-   **Focus:** Observe **Runge’s phenomenon** — polynomials can
    overshoot/oscillate (especially near boundaries) when trying to
    capture curved relationships.\
-   **Method:** For each degree, fit
    `lm(Sources ~ poly(Depth, degree, raw = TRUE))`, predict over the
    same grid `newd` used for the GAM, and collect fitted values/SEs.\
-   **Visual:** Overlay the polynomial fits on the raw data to compare
    shapes and **edge behaviour** with the GAM curve from B2.

**Key ideas:** - **Global vs local flexibility:** Global polynomials
force one equation to capture the entire curve; GAM smooths adapt
locally with a penalty on wiggliness.\
- **Edge effects:** Extrapolation/oscillation at the **depth extremes**
is common with higher-degree polynomials (Runge-type behaviour).\
- **Model selection:** AIC can be computed for each polynomial, but
shape realism and diagnostics matter just as much as a single metric.
:::

```{r}
#| label: B3-poly-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit polynomial regressions (degrees 2–6) and compare shapes to the GAM

# 0) Ensure you have a prediction grid (from Task B2); if not, create it:
# newd <- tibble(Depth = seq(min(___$___), max(___$___), length.out = ___))

# 1) Choose degrees to compare
deg_seq <- 2:6   # e.g., 2:6

# 2) Fit polynomials, predict on 'newd', and collect results
fits_poly <- map_df(deg_seq, ~{
  fm   <- lm(Sources ~ poly(Depth, .x, raw = TRUE), data = dat)
  pred <- predict(fm, newd, se.fit = TRUE)
  tibble(
    Depth = newd$Depth,
    fit   = as.numeric(pred$fit),
    se    = pred$se.fit,
    deg   = paste0("poly d=", .x),
    AIC   = AIC(fm)[1]
  )
})

# 3) Plot: raw data + polynomial fits
ggplot(dat, aes(Depth, Sources)) +
  geom_point(alpha = 0.3) +
  geom_line(data = fits_poly, aes(y = fit, colour = deg), linewidth = 0.7) +
  labs(title = "Polynomial fits of increasing degree",
       subtitle = "Watch for overshoot/oscillation at the edges (Runge-like behaviour)",
       colour = NULL) +
  theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **Overshoot zones:** At which depth ranges do the higher-degree
    polynomials deviate most from the data (overshoot/oscillation)?
    -   Mostly at the higher depths (3000-5000), but also a little at
        very low depths for the highest degree polynomials.
-   **Edge stability:** How does the **GAM** behave at
    shallowest/deepest depths compared with polynomials of degree ≥ 4?
    -   The GAM is much more stable at the ends, while the polynomials
        oscillate more and also predict values under 0.
-   **Parsimony vs realism:** If a degree-5 polynomial shows a slightly
    lower AIC than degree-3, would you still prefer the GAM? Why?
    -   I would still prefer the GAM, as the polynomials do not seem to
        get significantly better fits with higher degrees, while the GAM
        can more accurately predict values while also penalizing
        overfitting.\
-   **Prediction risk:** What practical risks arise if stakeholders act
    on edge-unstable predictions (e.g., setting thresholds at range
    limits)?
    -   There is a high risk of these values being plainly wrong or very
        incorrect, so acting on these values would end in unpredictable
        results.
:::

## Part C — How GAMs work (inner workings, lightly) (10 min)

::: {.alert .alert-info}
**Task C1 (concept).**

In `mgcv`, $f(x)$ is built from spline **basis functions** with a
smoothness penalty ($\lambda$).

Optimal $\lambda$ is estimated by REML or GCV; larger
$\lambda \Rightarrow$ smoother curve. **edf** (in `summary()`) reflects
the effective flexibility.
:::

::: {.alert .alert-info}
**Task C2 (hands-on) — Vary the basis dimension `k` to confirm
adequacy**

**What you’ll do (and why):**

-   **Purpose:** Check whether the chosen spline **basis size** (`k`) is
    large enough to capture the signal without imposing an artificial
    ceiling on model flexibility.\
-   **How it works:** In `mgcv`, `k` sets the **maximum** complexity of
    the smooth; the model then estimates the **effective degrees of
    freedom (edf)** via REML/GCV. Typically, `edf ≤ k - 1` for cubic
    regression splines.\
-   **Underspecification risk:** If `k` is too small, the smoother may
    be **forced** to be too simple even if the penalty would allow more
    wiggle—this leads to biased fits.\
-   **Diagnostics:** Use `gam.check()`’s **k-index** test. Warnings (low
    k-index and small p-values) suggest increasing `k` and refitting. If
    increasing `k` does **not** change the fitted curve or edf
    materially, the smaller `k` is sufficient.\
-   **Practical tip:** Start modest (e.g., `k=10`), increase to a larger
    value (e.g., `k=30`), compare `edf`, AIC, and residual diagnostics.
    With REML, a **too-large** `k` is usually safe (penalty controls
    overfitting), but avoid excessively huge `k` for computation and
    concurvity stability.
:::

```{r}
#| label: C2-k-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Compare k=10 vs k=30, inspect edf/AIC, and check k-index in gam.check()

# 1) Fit two GAMs with different basis dimensions
m_k10 <- gam(Sources ~ s(Depth, bs = "cr", k = 10), data = dat, method = "REML")
plot(m_k10)
m_k30 <- gam(Sources ~ s(Depth, bs = "cr", k = 30), data = dat, method = "REML")
plot(m_k30)

# 2) Compare model summaries (edf, R^2, etc.) at a glance
bind_rows(
  broom::glance(m_k10) %>% dplyr::mutate(model = "k=10"),
  broom::glance(m_k30) %>% dplyr::mutate(model = "k=30")
) %>%
  dplyr::select(model, df = df.residual, AIC, deviance, r.squared = adj.r.squared)

# 3) Diagnostics: residual patterns & basis adequacy (k-index)
par(mfrow = c(2, 2))
gam.check(m_k10)
par(mfrow = c(2, 2))
gam.check(m_k30)
par(mfrow = c(1, 1))

# Optional (for visual comparison): overlay fitted curves
# newd <- tibble(Depth = seq(min(___$___), max(___$___), length.out = 400))
# pred10 <- dplyr::bind_cols(newd, as_tibble(predict(m_k10, newd, se.fit = TRUE, type = "response")))
# pred30 <- dplyr::bind_cols(newd, as_tibble(predict(m_k30, newd, se.fit = TRUE, type = "response")))
# ggplot(dat, aes(Depth, Sources)) +
#   geom_point(alpha = 0.5) +
#   geom_line(data = pred10, aes(y = fit), linewidth = 0.9, linetype = 2) +
#   geom_line(data = pred30, aes(y = fit), linewidth = 1)

```

::: {.alert .alert-success}
**Questions**

-   **edf stability:** Do `edf` and the fitted curve change meaningfully
    when moving from `k=10` to `k=30`? What does that imply?
    -   The fitted curve and edf changes slightly from k=10 to k=30.
        This implies that all the extra allowed curvature is not
        actually used in the k=30 model, which further implies that the
        actual pattern in the data is not that more complex than the
        k=10 model suggests.\
-   **k-index check:** Does `gam.check()` flag basis inadequacy (k-index
    test)? If yes, what new `k` would you try next, and why?
    -   The k-index is over 1 with a high p-value, so increasing k is
        not needed (but doubling it is often a good strategy to see
        relative changes in model fit)\
-   **Parsimony vs safety:** Given similar fits, which `k` would you
    keep for subsequent analyses, and what’s your rationale?
    -   If increasing k does not increase fit, i would keep the simplest
        model, as it is often easier to interpret and analyse, and also
        avoids overfitting.\
-   **Computational trade-offs:** When might an unnecessarily large `k`
    become problematic, even if REML penalisation keeps the curve
    smooth?
    -   Very large k values take a lot of time to compute without any
        benefit.
:::

## Part D — GAMs vs GLMs (what changes?) (10 min)

::: {.alert .alert-info}
**Task D1 — Fit a GLM (Gaussian identity) and compare with the GAM**

**What you’ll do (and why):**

-   **Baseline model:** Fit a **GLM with Gaussian errors and identity
    link**: `Sources ~ Depth`. This is the linear benchmark against
    which we compare the GAM.\
-   **Model comparison:** Use **AIC** to compare `m_glm` vs `m_gam`.
    Lower AIC indicates a better trade-off between fit and complexity.\
-   **Diagnostics:** Inspect residuals of **both** models. Even with
    Gaussian errors, a linear mean structure can be misspecified if the
    *shape* is non-linear; GAMs can capture this via a smooth
    `s(Depth)`.\
-   **Takeaway:** A GAM can outperform a linear GLM under Gaussian
    errors when the **systematic component** (mean–covariate relation)
    is curved.

**Practical tips:** - A lower AIC for the GAM alongside cleaner
residuals supports non-linearity in the mean.\
- If AICs are close, prioritise **diagnostics** and **interpretability**
(edge behaviour, residual structure).
:::

```{r}
#| label: D1-glm-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit a Gaussian-identity GLM and compare with the GAM via AIC; inspect residuals.

# 1) Fit the baseline GLM (linear mean)
m_glm <- glm(Sources ~ Depth, data = dat, family = gaussian())

# 2) Compare to GAM via AIC
AIC(m_glm, m_gam)

# 3) Quick residual diagnostics (GLM): residuals–fitted and QQ
par(mfrow = c(1, 2))
plot(m_glm$fitted.values, m_glm$residuals, main = "GLM: Residuals vs Fitted",
     xlab = "Fitted", ylab = "Residuals"); abline(h = 0, lty = 2)
qqnorm(m_glm$residuals, main = "GLM: Normal Q-Q"); qqline(m_glm$residuals)
par(mfrow = c(1, 1))

# 4) (Optional) Re-run GAM diagnostics here for side-by-side judgement
# par(mfrow = c(2, 2)); gam.check(___); par(mfrow = c(1, 1))
```

:: {.alert .alert-success} **Questions**

-   **AIC verdict:** Which model has the lower AIC? Is the difference
    large enough to be compelling (\> \~2–4)?
    -   The GAM has the lowest AIC with a difference of \~100.\
-   **Residual patterns:** Do you see curvature in **GLM** residuals vs
    fitted values that disappears in the **GAM**?
    -   There is very strong curvature in the GLM residuals, which the
        GAM residuals do not have.\
-   **Model misspecification:** Why can a linear GLM be inadequate even
    if Gaussian error assumptions roughly hold?
    -   Because a GLM cannot accomodate non-linear (curved)
        relationsships that change over the range of predictor values.\
-   **Communication:** How would you explain to stakeholders why a
    smooth (GAM) is preferable here over a straight line? :::
    -   A GAM captures the more nuanced details of the pattern, that a
        GLM would miss and even wrongly describe: a GLM would not
        describe the hump or plateau very well compared to the GAM. The
        GAM is simply more accurate.

## Part E — Model fit & selection (15 min)

::: {.alert .alert-info}
**Task E1 — Compare alternative GAMs: different bases and smoothness
criteria**

**What you’ll do (and why):**

-   **Purpose:** To explore how GAM fits depend on the **choice of
    spline basis** and the **method for smoothing parameter
    estimation**.\
-   **Spline bases compared:**
    -   `"cr"` = **cubic regression spline** — efficient and common
        default.\
    -   `"tp"` = **thin plate regression spline** — more flexible,
        recommended when smoothness complexity is unknown.\
    -   `"cs"` = **cubic regression spline with shrinkage** — like
        `"cr"`, but allows the smoother to shrink towards 0 if
        unsupported by data (useful in model selection).\
-   **Smoothing selection methods:**
    -   **REML (Restricted Maximum Likelihood):** generally more stable
        and conservative.\
    -   **GCV.Cp (Generalised Cross Validation):** can be faster but
        sometimes under-smooths.\
-   **Approach:** Fit three models with the same data but different
    combinations of basis and smoothing method, then compare them using
    **AIC**.

**Key message:** If the AIC values and fitted curves are similar, the
choice of basis/method is less critical. Differences may matter in small
samples or when doing **variable selection**.
:::

```{r}
#| label: E1-compare-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Compare GAMs with different spline bases and smoothing selection criteria

# 1) Fit GAMs with different bases and methods
# Fit three GAMs with different bases/methods
# Cubic regression spline with REML smoothing parameter selection
m_cr_reml <- gam(Sources ~ s(Depth, bs = "cr"), data = dat, method = "REML")
# Thin plate regression spline with GCV.Cp smoothing parameter selection  
m_tp_gcv  <- gam(Sources ~ s(Depth, bs = "tp"), data = dat, method = "GCV.Cp")
# Cubic regression spline with shrinkage and REML smoothing parameter selection
m_cs_reml <- gam(Sources ~ s(Depth, bs = "cs"), data = dat, method = "REML")   # cubic shrinkage

# 2) Compare the models using AIC
AIC(m_cr_reml, m_tp_gcv, m_cs_reml)

```

::: {.alert .alert-info}
**Task E2 — Diagnostics and concurvity**

**What you’ll do (and why):**

-   **Residual diagnostics:** Use `gam.check()` to assess whether the
    model assumptions hold. This produces:
    -   Residuals vs fitted values (should show no clear pattern).\
    -   Q-Q plot of residuals (should be close to the line if residuals
        are Gaussian).\
    -   Scale–location plot (homogeneity of variance).\
    -   Histogram of residuals.\
    -   **k-index test** for basis dimension adequacy (low k-index
        suggests `k` may be too small).\
-   **Concurvity diagnostics:** Use `concurvity()` to check for strong
    correlations between smooth terms.
    -   Concurvity is like multicollinearity but for smooths.\
    -   Values close to 1 indicate redundancy between smooths, which can
        make interpretation unstable.\
    -   With only one smooth term (as in this example), concurvity isn’t
        a problem but running it introduces the concept for when you add
        more predictors.

**Key message:** A GAM is only trustworthy if residual diagnostics look
reasonable and concurvity is low. Always validate your model fit before
interpreting smooths.

**Decision prompt.** Pick a “best” model using AIC + diagnostics +
interpretability. State your choice and why (method, edf, residuals).
:::

```{r}
##| label: E2-diag-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Run model diagnostics (gam.check) and assess concurvity

# 1) Residual diagnostics for the chosen model
gam.check(m_cr_reml)

# 2) Concurvity assessment (more relevant with multiple smooths)
concurvity(m_cr_reml, full = TRUE)

# (add-on) — Visual residuals–fitted overlay with ggplot2**
## **What you’ll do (and why):**
## - Create a tidy data frame of **fitted values** (on the response scale) and **Pearson residuals** from your chosen GAM (e.g. `m_cr_reml`).
## - Plot **residuals vs fitted** using `ggplot2`, add a **zero reference line**, and a light **LOESS trend** to spot structure.
## - This complements `gam.check()` with a modern diagnostic visual and makes it easy to layer aesthetics if needed (e.g., colour by a factor).
# Goal: Build a ggplot residuals–fitted overlay for your chosen GAM (e.g., m_cr_reml)
# 1) Create a small diagnostics tibble with fitted values and Pearson residuals
#diag_df <- tibble(
#  fitted = fitted(___, type = "___"),
#  resid  = residuals(___, type = "___")
#)

# 2) Plot residuals vs fitted with a LOESS guide
#ggplot(diag_df, aes(x = ___, y = ___)) +
#  geom_hline(yintercept = 0, linetype = 2) +
#  geom_point(alpha = ___) +
#  geom_smooth(method = "loess", se = FALSE, span = 0.8) +
#  labs(title = "Residuals vs Fitted (GAM)",
#       x = "Fitted values",
#       y = "Pearson residuals") +
#  theme_minimal()

```

::: {.alert .alert-success}
**Questions**

-   **Residuals:** Do the residual plots suggest non-linearity,
    heteroscedasticity, or non-normality?
    -   The plots suggests non-normality, and some degree of
        heteroscedasticity\
-   **k-index:** Does `gam.check()` suggest that the basis size `k` was
    adequate? If not, what would you try next?
    -   the k-index is 1.04 with a high p-value so the basis k is good.
        If the k-index was under 1, I would increase k incrementally
        until a higher k-index is reached.\
-   **Concurvity:** Why is concurvity not an issue here, but critical
    when you have multiple smooth terms?
    -   This model only has one smooth term, and concurvity only happens
        with multiple smooth terms that can explain eachother with their
        smooth terms - this is a problem, as the model cannot determine
        which of the smooth terms has the actual effect.\
-   **Practice:** How would high concurvity affect your interpretation
    of a GAM with two correlated covariates?
    -   The predictions themselves would still be useful, but in terms
        of explaining which predictor (and how much) is driving the
        pattern in the reponse is difficult.\
:::

## Part F — Beyond a single station: Station effect + one smooth (15 min)

::: {.alert .alert-info}
**Task F1 — Two-station example with a shared smooth and a station
shift**

**What you’ll do (and why):**

-   **Purpose:** Combine Stations **8** and **13** and model a **common
    depth effect** via a single smooth `s(Depth)` while allowing a
    **parametric shift** between stations (an intercept difference via
    `+ Station`).\
-   **Depth overlap:** Restrict both stations to the **shared depth
    range** so that the common smooth isn’t biased by non-overlapping
    extremes (a classic source of spurious differences).\
-   **Model:** `Sources ~ s(Depth, bs="cr") + Station` with **REML**
    smoothing selection.\
-   **Inference:** Use `summary()` and `anova()` to assess:
    -   the **shape and significance** of `s(Depth)` (edf, F, p)\
    -   whether **Station** has a significant **offset** (parametric
        coefficient)

**Key idea:** This tests whether stations differ mainly by a **level
shift** while **sharing the same curve shape** with depth. If the offset
is insufficient (i.e., shapes appear different), move to **by-smooths**
in Task F2 (e.g., `s(Depth, by = Station) + Station`).
:::

```{r}
#| label: F1-two-stations-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit a GAM with a shared smooth over Depth and a parametric Station shift

# 1) Subset Stations 8 and 13; keep key variables
d2 <- ISIT |>
  dplyr::filter(Station %in% c(8, 13)) |>
  dplyr::transmute(Station = factor(Station),
                   Depth   = SampleDepth,
                   Sources = Sources)

# 2) Restrict to shared Depth overlap
rng <- d2 |>
  dplyr::group_by(Station) |>
  dplyr::summarise(mn = min(Depth), mx = max(Depth), .groups = "drop")
lo  <- max(rng$mn); hi <- min(rng$mx)
d2o <- d2 |>
  dplyr::filter(Depth > lo, Depth < hi)

# 3) Fit shared-smooth + station-offset model
m_two <- mgcv::gam(Sources ~ s(Depth, bs = "cr") + Station, data = d2, method = "REML")

# 4) Inspect results
summary(m_two)
anova(m_two)

```

::: {.alert .alert-success}
**Questions**

-   **Offset vs shape:** Does the station effect look like a **constant
    shift** across depths, or do you suspect **shape differences**?
    -   As the modelled is defined now, the station effect will have a
        constant shift effect across depths - there is no interaction
        between station and depth (which would result in different
        shapes)\
-   **Shared range:** How might failing to restrict to the **common
    depth range** distort conclusions?
    -   Differences between stations may be over- or underestimated, as
        there may be no comparable values across depth for the two
        stations.\
-   **Evidence:** What do the **edf** and p-values for `s(Depth)`
    suggest about non-linearity?
    -   edf is well above 1 with a very low p-value, which suggests no
        linearity, but a curved/wiggly effect.\
-   **Next step:** If the station offset is significant but residual
    plots show structure by station, what model term would you add next
    (and why)?
    -   I would add an interaction between station and depth, as the
        residual structures suggest different effects of different
        stations - an interaction.
:::

::: {.alert .alert-info}
**Task F2 — Plot the common smooth with a station offset**

**What you’ll do (and why):** - **Purpose:** Visualise the fitted
**shared smooth over Depth** with a **station-specific intercept shift**
from **Task F1**. This lets you see whether a single curve shape plus a
vertical offset captures both stations adequately.\
- **Prediction grid:** Build a tidy grid over the **common depth range**
(`d2o`) for **each station** so the offset appears as parallel curves
with identical shapes.\
- **Uncertainty:** Use `predict(..., se.fit = TRUE, type = "response")`
to obtain fitted values and pointwise SEs on the **response scale**.
Plot ribbons as \~95% intervals (`fit ± 2*se.fit`).\
- **Visual cues:** Scatter the raw data by station, overlay the
station-wise fitted lines, and add ribbons to compare fit quality across
stations and depths.

**Key idea:** If the curves appear **parallel** (same shape, vertical
shift), the shared-smooth + station-offset model is appropriate. If
shapes visibly diverge, consider **by-smooths**
(`s(Depth, by = Station) + Station`).
:::

```{r}
#| label: F2-plot-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Plot the common smooth + station offset with uncertainty bands

# 1) Build a prediction grid over the common depth range for each station
new2 <- tidyr::expand_grid(
  Depth   = seq(min(d2o$Depth), max(d2o$Depth), length.out = 400),
  Station = levels(d2o$Station)
)

# 2) Predict on the response scale with standard errors
p2 <- dplyr::bind_cols(
  new2,
  as_tibble(predict(m_two, new2, se.fit = TRUE, type = "response"))
)

# 3) Plot raw data + fitted lines + ribbons
ggplot(d2o, aes(Depth, Sources, colour = Station)) +
  geom_point(alpha = 0.3) +
  geom_line(data = p2, aes(y = fit)) +
  geom_ribbon(data = p2,
              aes(y = fit, 
                  ymin = fit - 2*se.fit,
                  ymax = fit + 2*se.fit,
                  fill = Station),
              alpha = 0.15, colour = NA) +
  labs(title = "GAM with shared smooth and station shift",
       subtitle = "Model: s(Depth) + Station") +
  theme_minimal() +
  guides(fill = "none")
```

::: {.alert .alert-success}
**Questions**

-   **Parallelism:** Do the two fitted lines look **parallel** across
    the depth range? If not, where do they diverge most?
    -   They look very parallel.\
-   **Uncertainty:** Where are the ribbons widest? Is that due to sparse
    data or genuine variability?
    -   The ribbons are widest at the hump and at the very end of the
        depth range. This could be due to genuine variability.\
-   **Fit adequacy:** Are there systematic zones where one station’s
    data consistently sits above/below the fitted line?
    -   At the hump- station 8's data is above the line, while station
        13's data is below the line.\
-   **Next model:** If shapes differ, how would **by-smooths** change
    the specification and interpretation?
    -   by-smooths would include the interaction effect between station
        and depth, and allow for individual effects of each station and
        interpretation of these effects.
:::

## Part G — Expanding GAMs: Beyond Gaussian (10 min)

::: {.alert .alert-info}
**Task G1 — Count responses: Poisson vs Negative Binomial GAMs +
overdispersion check**

**What you’ll do (and why):** - **Motivation:** `Sources` are
non-negative counts. Gaussian GAMs can fit the mean curve but ignore
count-like mean–variance structure.\
- **Families:**\
- **Poisson**: $\mathbb{E}[Y]=\mu$, $\mathrm{Var}(Y)=\mu$
(equidispersion).\
- **Negative Binomial (NB)**: $\mathrm{Var}(Y)=\mu + \mu^2/\theta$
(extra-Poisson/overdispersion handled by $\theta$).\
- **Approach:** Fit `Poisson` and `NB` GAMs with the same smoother as
before, compare **AIC**, and check **overdispersion** (Poisson).\
- **Overdispersion rule of thumb:** If
$\text{ratio} = \frac{\text{residual deviance}}{\text{df residual}} \gtrsim 1.5\!-\!2$,
Poisson is likely too restrictive → prefer **NB**.\
- **Outcome:** You’ll decide whether a count family yields better
fit/diagnostics than the Gaussian GAM.

**Notes:**\
- `type="response"` predictions keep the plot in data units.\
- With counts and small means, consider using **Pearson** residuals for
checks.
:::

```{r}
# Round the Sources to force a count
#| label: G1-counts-student
#| echo: true
#fill in the blanks (___) and then set eval: true to run
# Goal: Fit Poisson and NB GAMs for count data, compare AIC, and check overdispersion


# Round the Sources to force a count
dat$rnd.Sources <- round(dat$Sources)
# 1) Fit count-family GAMs with the same smoother used before
m_pois <- mgcv::gam(rnd.Sources ~ s(Depth, bs = "cr"), data = dat, family = poisson(), method = "REML")
m_nb   <- mgcv::gam(rnd.Sources ~ s(Depth, bs = "cr"), data = dat, family = nb(),    method = "REML")
gam.check(m_pois)
gam.check(m_nb)

# 2) Compare AICs (Gaussian vs Poisson vs NB)
AIC(m_gam, m_pois, m_nb)

# 3) Quick Poisson overdispersion checks
# (a) Residual deviance / df
with(broom::glance(m_pois), deviance / df.residual)

# (b) Pearson chi-square / df (often more informative)
chi_phi <- sum(residuals(m_pois, type = "pearson")^2) / df.residual(m_pois)
chi_phi

# OPTIONAL: Visual residuals–fitted overlay for NB model
# diag_nb <- tibble(fitted = fitted(___, type = "response"),
#                   resid  = residuals(___, type = "pearson"))
# ggplot(diag_nb, aes(fitted, resid)) +
#   geom_hline(yintercept = 0, linetype = 2) +
#   geom_point(alpha = 0.6) +
#   geom_smooth(method = "loess", se = FALSE, span = 0.8) +
#   labs(title = "Residuals vs Fitted (NB GAM)",
#        x = "Fitted (response scale)", y = "Pearson residuals") +
#   theme_minimal()
```

::: {.alert .alert-success}
**Questions**

-   **AIC & diagnostics:** Do Poisson/NB improve AIC versus the Gaussian
    GAM? What happens to residual patterns?
    -   Yes they have lower AIC values. The residual patterns provide
        more support for the poisson and nb models.\
-   **Dispersion:** What is the Poisson dispersion ratio? If high, how
    does NB change the picture?
    -   It is a meaure of overdispersion - here it is 0.70 (or 0.58),
        which is below 1.5-2, so there is no overdispersion. NB can
        account for this.\
-   **Interpretation:** How do you explain (to non-statisticians) the
    practical difference between **GAM (Gaussian)** and **GAM
    (Poisson/NB)** for these data?
    -   The two GAM models use the same underlying framework, but are
        adjusted to work with different data types and their underlying
        error distributions. The Poisson/NB GAM provide a better fitted
        model, and can therefore better describe the pattern in the
        data.\
-   **Next steps:** If NB still shows structure, what else might you try
    (offsets, additional covariates, alternative links, zero-inflation,
    etc.)?
    -   Adding predictors might be able to explain some of the
        unexplained variance.

    -   Adding interactions might be more accurate than having the
        effects of predictors separated.

    -   Zero-inflation models can account for too many 0-values
:::

## Summary of the Practical

This practical introduced students to the use of Generalised Additive
Models (GAMs) through the bioluminescence depth dataset. We began by
visualising raw data at single stations, highlighting the inadequacy of
linear fits and motivating the need for flexible smoothers. By fitting
cubic regression splines in mgcv::gam, students explored the principle
of replacing fixed parametric terms with smooth functions to capture
non-linear relationships.

We contrasted GAMs with polynomial regressions, which can mimic
non-linearity but suffer from instability at range edges (Runge’s
phenomenon). This emphasised why GAMs are preferable: they avoid
oscillations, allow local flexibility, and are regularised through
penalisation. The role of the basis dimension (k) and smoothing
parameter (λ) was examined, with diagnostic tools (gam.check, residual
plots, k-index) showing how to assess adequacy and avoid
under/over-smoothing.

We extended the models by comparing different spline bases (cubic
regression, thin-plate, shrinkage) and smoothness criteria (REML, GCV).
Students practised evaluating model fit with AIC and concurvity checks,
reinforcing that GAMs retain the statistical rigour of GLMs while
offering flexibility. A comparison between GLMs and GAMs (Gaussian
identity) demonstrated why GAMs often outperform when relationships are
non-linear, even with the same error family.

Building complexity, we modelled multiple stations with shared smooths
and parametric shifts, and discussed when by-smooths are preferable.
Finally, the practical illustrated GAMs beyond Gaussian responses,
fitting Poisson and Negative Binomial GAMs to count data, and
highlighting the importance of overdispersion checks.

Overall, the session showed that GAMs form a natural extension of
regression models:

-   From a single smoother (non-linear regression replacement),
-   Through diagnostics and model selection,
-   To flexible multi-station and non-Gaussian applications.

::: callout-note
**Key Take-Home Points** \* GAMs extend GLMs by allowing smooth,
data-driven functional forms for predictors. \* Polynomials $\neq$
smoothers: GAMs avoid Runge’s phenomenon and handle local variation
better. \* Diagnostics matter: always check gam.check, AIC, and
concurvity before trusting results. \* Flexible families: GAMs work with
Gaussian, Poisson, binomial, NB, and beyond. \* Interpretation: Smooth
terms represent shapes, not single coefficients — describe curves, not
slopes.
:::
